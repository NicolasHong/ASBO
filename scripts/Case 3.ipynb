{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "BASE_DIR = Path().absolute().parent\n",
    "sys.path.append(str(BASE_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt_adaptive  import BayesianOptimization\n",
    "from bayes_opt_adaptive import UtilityFunction\n",
    "from scipy.optimize import minimize, NonlinearConstraint, Bounds, SR1\n",
    "from bayes_opt_adaptive.logger import JSONLogger\n",
    "from bayes_opt_adaptive.event import Events\n",
    "from bayes_opt_adaptive.util import load_logs\n",
    "from cases import Case1,Case2,Case3\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = BASE_DIR/'scripts_res'\n",
    "DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_run = True\n",
    "re_run_save = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol = 'ASBO'\n",
    "case_name = 'Case3'\n",
    "runs_number = 3\n",
    "\n",
    "obj_list = []\n",
    "x_1_list = []\n",
    "x_2_list = []\n",
    "x_3_list = []\n",
    "x_4_list = []\n",
    "x_5_list = []\n",
    "con_1_list  = []\n",
    "con_2_list  = []\n",
    "con_3_list  = []\n",
    "con_4_list  = []\n",
    "\n",
    "def func(x1,x2,x3,x4,x5):\n",
    "    x = np.array([x1,x2,x3,x4,x5])\n",
    "    obj, c1, c2, c3, c4= Case3(x)\n",
    "    cons = [c1,c2,c3,c4]\n",
    "    return -obj,cons\n",
    "\n",
    "nc_lb = np.array([-1,-1,-2,0,])\n",
    "nc_ub = np.array([2,  4, 3,1.5])\n",
    "cons = NonlinearConstraint(func, nc_lb, nc_ub)\n",
    "pbounds = {'x1': (-5, 12),'x2': (-20, 22),'x3': (-10, 10),'x4': (-12, 20),'x5': (-15, 18),}\n",
    "acquisition_function = UtilityFunction(kind=\"ucb\", kappa=2.5, xi=0, kappa_decay=1, kappa_decay_delay=0)\n",
    "\n",
    "if re_run == True or re_run_save == True:\n",
    "    for i in range(runs_number):\n",
    "        optimizer = BayesianOptimization(f=func, pbounds=pbounds,constraint=cons,verbose=0,random_state=i)\n",
    "        optimizer.maximize(init_points=100, n_iter=200,p_hyper=[[5, 2, 0.78, 0.95,1e-5]],acquisition_function=acquisition_function)\n",
    "        print(optimizer.max)\n",
    "        obj_list.extend([optimizer.max['target']])\n",
    "        x_1_list.extend([optimizer.max['params']['x1']])\n",
    "        x_2_list.extend([optimizer.max['params']['x2']])\n",
    "        x_3_list.extend([optimizer.max['params']['x3']])\n",
    "        x_4_list.extend([optimizer.max['params']['x4']])\n",
    "        x_5_list.extend([optimizer.max['params']['x5']])                        \n",
    "        con_1_list.extend([optimizer.max['constraint'][0]])\n",
    "        con_2_list.extend([optimizer.max['constraint'][1]])\n",
    "        con_3_list.extend([optimizer.max['constraint'][2]])\n",
    "        con_4_list.extend([optimizer.max['constraint'][3]])\n",
    "        df = pd.DataFrame({\n",
    "        'obj': obj_list,\n",
    "        'x_1': x_1_list,\n",
    "        'x_2': x_2_list,\n",
    "        'x_3': x_3_list,\n",
    "        'x_4': x_4_list,\n",
    "        'x_5': x_5_list,\n",
    "        'con_1': con_1_list,\n",
    "        'con_2': con_2_list,\n",
    "        'con_3': con_3_list,\n",
    "        'con_4': con_4_list\n",
    "        })\n",
    "        if re_run_save == True:\n",
    "            df.to_csv(DIR/f'{sol}_Results_{case_name}_{runs_number}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_count = df['obj'].isna().sum()\n",
    "print(f\"NaN count in column 'obj': {nan_count}\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trust-constr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_run = True\n",
    "re_run_save = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol = 'trust-constr'\n",
    "list_realvalue = []\n",
    "case_name = 'Case3'\n",
    "runs_number = 3\n",
    "\n",
    "def func(x):\n",
    "    obj, c1, c2, c3, c4 = Case3(x)\n",
    "    cons = [c1,c2,c3,c4]\n",
    "    list_realvalue.extend([[obj]+cons])\n",
    "    return obj,c1,c2,c3,c4\n",
    "\n",
    "initial_x1 = []\n",
    "initial_x2 = []\n",
    "initial_x3 = []\n",
    "initial_x4 = []\n",
    "initial_x5 = []\n",
    "\n",
    "x_1_list = []\n",
    "x_2_list = []\n",
    "x_3_list = []\n",
    "x_4_list = []\n",
    "x_5_list = []\n",
    "obj_list = []\n",
    "con_1_list  = []\n",
    "con_2_list  = []\n",
    "con_3_list  = []\n",
    "con_4_list  = []\n",
    "\n",
    "nfev_r = []\n",
    "success_r = []\n",
    "message_r = []\n",
    "def recyc(x):\n",
    "    nc_lb = np.array([-1,-1,-2,0,])\n",
    "    nc_ub = np.array([2,  4, 3,1.5])\n",
    "    cons = NonlinearConstraint(lambda x: func(x)[1:5], nc_lb, nc_ub, hess=SR1(), finite_diff_rel_step=0.001)\n",
    "    objective = lambda x: func(x)[0]\n",
    "    x0=x\n",
    "    lb = [-5, -20,-10,-12,-15] \n",
    "    ub = [12,22,10,20,18] \n",
    "    bd = Bounds(lb, ub, True) \n",
    "    result = minimize(objective, x0,  method='trust-constr', bounds=bd,constraints=cons)\n",
    "    print('obj = ', result.fun, ', x = ', result.x)\n",
    "    obj_list.append(result.fun)\n",
    "    x_1_list.append(result.x[0])\n",
    "    x_2_list.append(result.x[1])\n",
    "    x_3_list.append(result.x[2])\n",
    "    x_4_list.append(result.x[3])\n",
    "    x_5_list.append(result.x[4])        \n",
    "    con_1_list.append(result.constr[0][0])\n",
    "    con_2_list.append(result.constr[0][1])\n",
    "    con_3_list.append(result.constr[0][2])    \n",
    "    con_4_list.append(result.constr[0][3])    \n",
    "    initial_x1.append(x[0][0])  \n",
    "    initial_x2.append(x[0][1])  \n",
    "    initial_x3.append(x[0][2])  \n",
    "    initial_x4.append(x[0][3])  \n",
    "    initial_x5.append(x[0][4])      \n",
    "    nfev_r.append(result.nfev)\n",
    "    success_r.append(str(result.success))\n",
    "    message_r.append(result.message)\n",
    "    df = pd.DataFrame({ \n",
    "        'obj': obj_list,\n",
    "        'x_1': x_1_list,\n",
    "        'x_2': x_2_list,\n",
    "        'x_3': x_3_list,\n",
    "        'x_4': x_4_list,\n",
    "        'x_5': x_5_list,\n",
    "        'con_1': con_1_list,\n",
    "        'con_2': con_2_list,\n",
    "        'con_3': con_3_list,\n",
    "        'con_4': con_4_list,\n",
    "        'initial_x1': initial_x1,\n",
    "        'initial_x2': initial_x2, \n",
    "        'initial_x3': initial_x3,\n",
    "        'initial_x4': initial_x4,  \n",
    "        'initial_x5': initial_x5,                 \n",
    "        'nfev': nfev_r,\n",
    "        'success': success_r,\n",
    "        'message': message_r,\n",
    "        })    \n",
    "    if re_run_save == True:\n",
    "        df.to_csv(DIR/f'{sol}_Results_{case_name}_{runs_number}.csv', index=False)\n",
    "    return result.x\n",
    "\n",
    "def initial_random(lb=[-5, -20,-10,-12,-15],ub=[12,22,10,20,18] ,a=2,n_size=runs_number):\n",
    "    bounds = np.array([lb,ub])\n",
    "    bounds = bounds.T\n",
    "    random_state = np.random.RandomState(a)\n",
    "    x_random = random_state.uniform(bounds[:, 0], bounds[:, 1],\n",
    "                                    size=(n_size, bounds.shape[0]))\n",
    "    return x_random\n",
    "\n",
    "if re_run == True or re_run_save == True:\n",
    "    x_random = initial_random(lb=[-5, -20,-10,-12,-15],ub=[12,22,10,20,18])\n",
    "    from itertools import product\n",
    "    for combination in product(x_random):\n",
    "        recyc(combination)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DIR/f'{sol}_Results_{case_name}_{runs_number}.csv')\n",
    "nan_count = df['obj'].isna().sum()\n",
    "print(f\"NaN count in column 'obj': {nan_count}\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### slsqp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_run = True\n",
    "re_run_save = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol = 'slsqp'\n",
    "case_name = 'Case3'\n",
    "runs_number = 3\n",
    "\n",
    "initial_x1 = []\n",
    "initial_x2 = []\n",
    "initial_x3 = []\n",
    "initial_x4 = []\n",
    "initial_x5 = []\n",
    "\n",
    "x_1_list = []\n",
    "x_2_list = []\n",
    "x_3_list = []\n",
    "x_4_list = []\n",
    "x_5_list = []\n",
    "obj_list = []\n",
    "\n",
    "nfev_r = []\n",
    "success_r = []\n",
    "message_r = []\n",
    "\n",
    "def func(x):\n",
    "    obj, c1, c2, c3, c4 = Case3(x)\n",
    "    cons = [c1,c2,c3,c4]\n",
    "    list_realvalue.extend([[obj]+cons])\n",
    "    return obj,c1,c2,c3,c4\n",
    "\n",
    "initial_x = []\n",
    "fun_r = []\n",
    "nfev_r = []\n",
    "success_r = []\n",
    "message_r = []\n",
    "def recyc(x):\n",
    "    nc_lb = np.array([-1,-1,-2,0,])\n",
    "    nc_ub = np.array([2,  4, 3,1.5])\n",
    "    cons = NonlinearConstraint(lambda x: func(x)[1:5], nc_lb, nc_ub, hess=SR1(), finite_diff_rel_step=0.001)\n",
    "    objective = lambda x: func(x)[0]\n",
    "    x0=x\n",
    "    lb = [-5, -20,-10,-12,-15] \n",
    "    ub = [12,22,10,20,18] \n",
    "    bd = Bounds(lb, ub, True) \n",
    "    result = minimize(objective, x0,  method='slsqp', bounds=bd,constraints=cons)\n",
    "    print('obj = ', result.fun, ', x = ', result.x)\n",
    "    obj_list.append(result.fun)\n",
    "    x_1_list.append(result.x[0])\n",
    "    x_2_list.append(result.x[1])\n",
    "    x_3_list.append(result.x[2])\n",
    "    x_4_list.append(result.x[3])\n",
    "    x_5_list.append(result.x[4])        \n",
    "\n",
    "    initial_x1.append(x[0][0])  \n",
    "    initial_x2.append(x[0][1])  \n",
    "    initial_x3.append(x[0][2])  \n",
    "    initial_x4.append(x[0][3])  \n",
    "    initial_x5.append(x[0][4])      \n",
    "    nfev_r.append(result.nfev)\n",
    "    success_r.append(str(result.success))\n",
    "    message_r.append(result.message)\n",
    "    df = pd.DataFrame({ \n",
    "        'obj': obj_list,\n",
    "        'x_1': x_1_list,\n",
    "        'x_2': x_2_list,\n",
    "        'x_3': x_3_list,\n",
    "        'x_4': x_4_list,\n",
    "        'x_5': x_5_list,\n",
    "        'initial_x1': initial_x1,\n",
    "        'initial_x2': initial_x2, \n",
    "        'initial_x3': initial_x3,\n",
    "        'initial_x4': initial_x4,  \n",
    "        'initial_x5': initial_x5,                 \n",
    "        'nfev': nfev_r,\n",
    "        'success': success_r,\n",
    "        'message': message_r,\n",
    "        })    \n",
    "    if re_run_save == True:\n",
    "        df.to_csv(DIR/f'{sol}_Results_{case_name}_{runs_number}.csv', index=False)\n",
    "    return result.x\n",
    "\n",
    "def initial_random(lb=[-5, -20,-10,-12,-15],ub=[12,22,10,20,18] ,a=2,n_size=runs_number):\n",
    "    bounds = np.array([lb,ub])\n",
    "    bounds = bounds.T\n",
    "    random_state = np.random.RandomState(a)\n",
    "    x_random = random_state.uniform(bounds[:, 0], bounds[:, 1],\n",
    "                                    size=(n_size, bounds.shape[0]))\n",
    "    return x_random\n",
    "\n",
    "if re_run == True or re_run_save == True:\n",
    "    x_random = initial_random(lb=[-5, -20,-10,-12,-15],ub=[12,22,10,20,18])\n",
    "    from itertools import product\n",
    "    for combination in product(x_random):\n",
    "        recyc(combination)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DIR/f'{sol}_Results_{case_name}_{runs_number}.csv')\n",
    "nan_count = df['obj'].isna().sum()\n",
    "print(f\"NaN count in column 'obj': {nan_count}\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_run = True\n",
    "re_run_save = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol = 'CBO'\n",
    "case_name = 'Case3'\n",
    "runs_number = 3\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt import UtilityFunction\n",
    "\n",
    "obj_list = []\n",
    "x_1_list = []\n",
    "x_2_list = []\n",
    "x_3_list = []\n",
    "x_4_list = []\n",
    "x_5_list = []\n",
    "con_1_list  = []\n",
    "con_2_list  = []\n",
    "con_3_list  = []\n",
    "con_4_list  = []\n",
    "\n",
    "def func_obj(x1,x2,x3,x4,x5):\n",
    "    x = np.array([x1,x2,x3,x4,x5])\n",
    "    obj, c1, c2, c3, c4= Case3(x)\n",
    "    cons = [c1,c2,c3,c4]\n",
    "    # print(('cons, obj value=', cons, obj))\n",
    "    return -obj\n",
    "\n",
    "def func_cons(x1,x2,x3,x4,x5):\n",
    "    x = np.array([x1,x2,x3,x4,x5])\n",
    "    obj, c1, c2, c3, c4= Case3(x)\n",
    "    cons = [c1,c2,c3,c4]\n",
    "    return cons\n",
    "\n",
    "nc_lb = np.array([-1,-1,-2,0,])\n",
    "nc_ub = np.array([2,  4, 3,1.5])\n",
    "cons = NonlinearConstraint(func_cons, nc_lb, nc_ub)\n",
    "pbounds = {'x1': (-5, 12),'x2': (-20, 22),'x3': (-10, 10),'x4': (-12, 20),'x5': (-15, 18),}\n",
    "acquisition_function = UtilityFunction(kind=\"ucb\", kappa=1)\n",
    "\n",
    "if re_run == True or re_run_save == True:\n",
    "    for i in range(runs_number):\n",
    "        optimizer = BayesianOptimization(f=func_obj, pbounds=pbounds,constraint=cons,verbose=0,random_state=i)\n",
    "        optimizer.maximize(init_points=100, n_iter=200,acquisition_function=acquisition_function)\n",
    "        print(optimizer.max)\n",
    "        obj_list.extend([optimizer.max['target']])\n",
    "        x_1_list.extend([optimizer.max['params']['x1']])\n",
    "        x_2_list.extend([optimizer.max['params']['x2']])\n",
    "        x_3_list.extend([optimizer.max['params']['x3']])\n",
    "        x_4_list.extend([optimizer.max['params']['x4']])\n",
    "        x_5_list.extend([optimizer.max['params']['x5']])                        \n",
    "        con_1_list.extend([optimizer.max['constraint'][0]])\n",
    "        con_2_list.extend([optimizer.max['constraint'][1]])\n",
    "        con_3_list.extend([optimizer.max['constraint'][2]])\n",
    "        con_4_list.extend([optimizer.max['constraint'][3]])\n",
    "        df = pd.DataFrame({\n",
    "        'obj': obj_list,\n",
    "        'x_1': x_1_list,\n",
    "        'x_2': x_2_list,\n",
    "        'x_3': x_3_list,\n",
    "        'x_4': x_4_list,\n",
    "        'x_5': x_5_list,        \n",
    "        'con_1': con_1_list,\n",
    "        'con_2': con_2_list,\n",
    "        'con_3': con_3_list,\n",
    "        'con_4': con_4_list\n",
    "        })\n",
    "        if re_run_save == True:\n",
    "            df.to_csv(DIR/f'{sol}_Results_{case_name}_{runs_number}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_count = df['obj'].isna().sum()\n",
    "print(f\"NaN count in column 'obj': {nan_count}\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TuRBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_run = True\n",
    "re_run_save = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol = 'TuRBO'\n",
    "case_name = 'Case3'\n",
    "\n",
    "temp_y = []\n",
    "temp_x = []\n",
    "x_1_list = []\n",
    "x_2_list = []\n",
    "x_3_list = []\n",
    "x_4_list = []\n",
    "x_5_list = []\n",
    "iterations = []\n",
    "max_values = []\n",
    "\n",
    "runs_number = 3\n",
    "itera = 0\n",
    "import gpytorch\n",
    "import torch\n",
    "import tr_bo\n",
    "from torch.quasirandom import SobolEngine\n",
    "from botorch.utils.transforms import unnormalize\n",
    "from botorch.models.model_list_gp_regression import ModelListGP\n",
    "\n",
    "def func(x):\n",
    "    global itera\n",
    "    obj, c1, c2, c3, c4 = Case3(x)\n",
    "    itera=itera+1\n",
    "\n",
    "    return -obj,c1,c2, c3,c4\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.double\n",
    "dim = 1\n",
    "batch_size = 4\n",
    "n_init = 100 \n",
    "max_cholesky_size = float(\"inf\")  # Always use Cholesky\n",
    "\n",
    "lb = torch.tensor([-1.0, -1, -2.0, 0,], dtype=torch.float64)\n",
    "ub =  torch.tensor([2.0, 4, 3.0, 1.5, ], dtype=torch.float64)\n",
    "bounds = torch.tensor([[-5, -20,  -10, -12, -15], [12, 22, 10, 20, 18]])\n",
    "\n",
    "def objective(x):\n",
    "    x_numpy = x.numpy()\n",
    "    res = func(x_numpy)\n",
    "    target = res[0]\n",
    "    c1 = res[1]\n",
    "    c2 = res[2]\n",
    "    c3 = res[3]\n",
    "    c4 = res[4]\n",
    "    return target,c1,c2,c3,c4\n",
    "def run_multiple_calculations(num_runs):\n",
    "    for random_seed in range(num_runs):\n",
    "        try:\n",
    "            def eval_objective(x):\n",
    "                return objective(unnormalize(x, bounds))\n",
    "            # Initialize TuRBO state\n",
    "            state = tr_bo.ScboState(dim, batch_size=batch_size)\n",
    "            sobol = SobolEngine(dim, scramble=True, seed=1)\n",
    "\n",
    "            # Get initial data\n",
    "            # Must get initial values for both objective and constraints\n",
    "            train_X = tr_bo.get_initial_points(dim, n_init,seed=random_seed)\n",
    "\n",
    "            train_Y_tensor_list = []\n",
    "            C1_tensor_list = []\n",
    "            C2_tensor_list = []\n",
    "            C3_tensor_list = []\n",
    "            C4_tensor_list = []\n",
    "            for x in train_X:\n",
    "                result = eval_objective(x)\n",
    "                combined_tensor = torch.tensor(result, dtype=dtype, device=device).unsqueeze(-1)\n",
    "                train_Y, C1, C2,C3,C4 = combined_tensor[0], combined_tensor[1], combined_tensor[2],combined_tensor[3], combined_tensor[4]\n",
    "                train_Y_tensor_list.append(train_Y)\n",
    "                C1_tensor_list.append(C1)\n",
    "                C2_tensor_list.append(C2)\n",
    "                C3_tensor_list.append(C3)\n",
    "                C4_tensor_list.append(C4)\n",
    "\n",
    "            train_Y = torch.stack(train_Y_tensor_list, dim=0)\n",
    "            C1 = torch.stack(C1_tensor_list, dim=0)\n",
    "            C2 = torch.stack(C2_tensor_list, dim=0)\n",
    "            C3 = torch.stack(C3_tensor_list, dim=0)\n",
    "            C4 = torch.stack(C4_tensor_list, dim=0)\n",
    "            while not state.restart_triggered:  # Run until TuRBO converges\n",
    "                # Fit GP models for objective and constraints\n",
    "                model = tr_bo.get_fitted_model(train_X, train_Y,dim)\n",
    "                c1_model = tr_bo.get_fitted_model(train_X, C1,dim)\n",
    "                c2_model = tr_bo.get_fitted_model(train_X, C2,dim)\n",
    "                c3_model = tr_bo.get_fitted_model(train_X, C3,dim)\n",
    "                c4_model = tr_bo.get_fitted_model(train_X, C4,dim)\n",
    "                # Generate a batch of candidates\n",
    "                with gpytorch.settings.max_cholesky_size(max_cholesky_size):\n",
    "                    X_next = tr_bo.generate_batch(\n",
    "                        state=state,\n",
    "                        model=model,\n",
    "                        X=train_X,\n",
    "                        Y=train_Y,\n",
    "                        batch_size=batch_size,\n",
    "                        n_candidates=2000,\n",
    "                        constraint_model=ModelListGP(c1_model, c2_model,c3_model,c4_model),\n",
    "                        sobol=sobol,\n",
    "                    )\n",
    "\n",
    "                Y_next_tensor_list = []\n",
    "                C1_next_tensor_list = []\n",
    "                C2_next_tensor_list = []\n",
    "                C3_next_tensor_list = []\n",
    "                C4_next_tensor_list = []\n",
    "                for x in X_next:\n",
    "                    result = eval_objective(x)\n",
    "                    combined_tensor = torch.tensor(result, dtype=dtype, device=device).unsqueeze(-1)\n",
    "                    Y_next, C1_next, C2_next, C3_next, C4_next,= combined_tensor[0], combined_tensor[1], combined_tensor[2],combined_tensor[3], combined_tensor[4]\n",
    "                    Y_next_tensor_list.append(Y_next)\n",
    "                    C1_next_tensor_list.append(C1_next)\n",
    "                    C2_next_tensor_list.append(C2_next)\n",
    "                    C3_next_tensor_list.append(C3_next)\n",
    "                    C4_next_tensor_list.append(C4_next)\n",
    "                Y_next = torch.stack(Y_next_tensor_list, dim=0)\n",
    "                C1_next = torch.stack(C1_next_tensor_list, dim=0)\n",
    "                C2_next = torch.stack(C2_next_tensor_list, dim=0)\n",
    "                C3_next = torch.stack(C3_next_tensor_list, dim=0)\n",
    "                C4_next = torch.stack(C4_next_tensor_list, dim=0)\n",
    "\n",
    "                C_next = torch.cat([C1_next, C2_next,C3_next, C4_next,], dim=-1)\n",
    "                state = tr_bo.update_state(state, Y_next, C_next,ub,lb)\n",
    "                train_X = torch.cat((train_X, X_next), dim=0)\n",
    "                train_Y = torch.cat((train_Y, Y_next), dim=0)\n",
    "                C1 = torch.cat((C1, C1_next), dim=0)\n",
    "                C2 = torch.cat((C2, C2_next), dim=0)\n",
    "                C3 = torch.cat((C3, C3_next), dim=0)\n",
    "                C4 = torch.cat((C4, C4_next), dim=0)\n",
    "                C = torch.cat([C1, C2, C3, C4], dim=-1)\n",
    "                bool_tensor = (C >= lb.repeat(len(C), 1)) & (C <= ub.repeat(len(C), 1))\n",
    "                bool_tensor = torch.all(bool_tensor, dim=-1)\n",
    "                Valid_Y = train_Y[bool_tensor]\n",
    "                Valid_X = train_X[bool_tensor]\n",
    "                if len(Valid_Y)==0:\n",
    "                    break\n",
    "                else:\n",
    "                    max_index = torch.argmax(Valid_Y)  \n",
    "                    max_y = Valid_Y[max_index]         \n",
    "                    corresponding_x = Valid_X[max_index]\n",
    "                    temp_y.append(max_y)\n",
    "                    temp_x.append(corresponding_x)\n",
    "                    # list_realvalue.append(max(Valid_Y[0]))   # Wrong version\n",
    "                if len(temp_y) == 50:\n",
    "                    max_value = max(temp_y).item()\n",
    "                    temp_y_np = np.array([t.item() for t in temp_y])\n",
    "                    max_index = np.argmax(temp_y_np)\n",
    "                    # max_index = np.argmax(temp_y)\n",
    "                    c_x = temp_x[max_index]\n",
    "                    print(\"obj= \",max_value, ' x = ', c_x)\n",
    "                    max_values.append(max_value)\n",
    "                    x_1_list.append(c_x[0].item())\n",
    "                    x_2_list.append(c_x[1].item())\n",
    "                    x_3_list.append(c_x[2].item())\n",
    "                    x_4_list.append(c_x[3].item())\n",
    "                    x_5_list.append(c_x[4].item())                                        \n",
    "                    if len(iterations)==0:\n",
    "                        iterations.append(itera)\n",
    "                    else:\n",
    "                        iterations.append(itera-sum(iterations))\n",
    "                    temp_y.clear()\n",
    "                    temp_x.clear()\n",
    "\n",
    "                    df = pd.DataFrame({\n",
    "                    'obj': max_values,\n",
    "                    'x_1': x_1_list,\n",
    "                    'x_2': x_2_list,\n",
    "                    'x_3': x_3_list,\n",
    "                    'x_4': x_4_list,\n",
    "                    'x_5': x_5_list,                       \n",
    "                    'iterations': iterations\n",
    "                    })\n",
    "                    if re_run_save == True:\n",
    "                        df.to_csv(DIR/f'{sol}_Results_{case_name}_{runs_number}.csv', index=False)\n",
    "                    break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            if len(temp_y)==0:\n",
    "                print(\"Early stop and no feasible solutions\")\n",
    "                max_values.append(None)\n",
    "                x_1_list.append(None)\n",
    "                x_2_list.append(None)\n",
    "                x_3_list.append(None)\n",
    "                x_4_list.append(None)\n",
    "                x_5_list.append(None)                \n",
    "            else:\n",
    "                max_value = max(temp_y).item()\n",
    "                temp_y_np = np.array([t.item() for t in temp_y])\n",
    "                max_index = np.argmax(temp_y_np)\n",
    "                # max_index = np.argmax(temp_y)\n",
    "                c_x = temp_x[max_index]\n",
    "                print(\"Early stop and feasible\")\n",
    "                print(\"obj= \",max_value, ' x = ', c_x)\n",
    "                \n",
    "                max_values.append(max_value)\n",
    "                x_1_list.append(c_x[0].item())\n",
    "                x_2_list.append(c_x[1].item())\n",
    "                x_3_list.append(c_x[2].item())\n",
    "                x_4_list.append(c_x[3].item())\n",
    "                x_5_list.append(c_x[4].item())                   \n",
    "            if len(iterations)==0:\n",
    "                iterations.append(itera)\n",
    "            else:\n",
    "                iterations.append(itera-sum(iterations))\n",
    "            temp_y.clear()\n",
    "            temp_x.clear()\n",
    "\n",
    "            df = pd.DataFrame({\n",
    "            'obj': max_values,\n",
    "            'x_1': x_1_list,\n",
    "            'x_2': x_2_list,\n",
    "            'x_3': x_3_list,\n",
    "            'x_4': x_4_list,\n",
    "            'x_5': x_5_list,               \n",
    "            'iterations': iterations\n",
    "            })\n",
    "            if re_run_save == True:\n",
    "                df.to_csv(DIR/f'{sol}_Results_{case_name}_{runs_number}.csv', index=False)\n",
    "\n",
    "run_multiple_calculations(num_runs=runs_number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DIR/f'{sol}_Results_{case_name}_{runs_number}.csv')\n",
    "nan_count = df['obj'].isna().sum()\n",
    "print(f\"NaN count in column 'obj': {nan_count}\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_run = True\n",
    "re_run_save = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol = 'PSO'\n",
    "case_name = 'Case3'\n",
    "\n",
    "x_1_list = []\n",
    "x_2_list = []\n",
    "x_3_list = []\n",
    "x_4_list = []\n",
    "x_5_list = []\n",
    "obj_list = []\n",
    "iterations=[]\n",
    "\n",
    "runs_number = 3\n",
    "itera = 0\n",
    "from pyswarm import pso\n",
    "\n",
    "def func(x):\n",
    "    global itera\n",
    "    obj, c1, c2, c3, c4 = Case3(x)\n",
    "    cons = [c1+1,2-c1,c2+1,4-c2,c3+2,3-c3,c4,1.5-c4]\n",
    "    global x_p\n",
    "    try:\n",
    "        equal = np.array_equal(x,x_p)\n",
    "        if not equal:\n",
    "            itera=itera+1\n",
    "    except:\n",
    "        itera=itera+1\n",
    "    x_p =x\n",
    "    return obj,cons\n",
    "\n",
    "if re_run == True or re_run_save == True:\n",
    "    for num in range(runs_number):\n",
    "        xopt, fopt = pso(lambda x:func(x)[0], lb =[-5, -20,-10,-2,-15] ,ub=[12,22,10,20,18],f_ieqcons=lambda x:func(x)[1:2], swarmsize=100, omega=0.5, phip=0.5, phig=0.5, maxiter=200, minstep=1e-8,minfunc=1e-8, debug=False)\n",
    "        print( 'fopt =', fopt, 'xopt = ', xopt)\n",
    "        if len(iterations)==0:\n",
    "            iterations.append(itera)\n",
    "        else:\n",
    "            iterations.append(itera-sum(iterations))\n",
    "        obj_list.append(fopt)\n",
    "        x_1_list.append(xopt[0])\n",
    "        x_2_list.append(xopt[1])\n",
    "        x_3_list.append(xopt[2])\n",
    "        x_4_list.append(xopt[3])\n",
    "        x_5_list.append(xopt[4])\n",
    "\n",
    "        df = pd.DataFrame({\n",
    "        'obj': obj_list,\n",
    "        'x_1': x_1_list,\n",
    "        'x_2': x_2_list,\n",
    "        'x_3': x_3_list,\n",
    "        'x_4': x_4_list,\n",
    "        'x_5': x_5_list,    \n",
    "        'iterations': iterations\n",
    "        })\n",
    "        if re_run_save == True:\n",
    "            df.to_csv(DIR/f'{sol}_Results_{case_name}_{runs_number}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_count = df['obj'].isna().sum()\n",
    "print(f\"NaN count in column 'obj': {nan_count}\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_run = True\n",
    "re_run_save = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol = 'GA'\n",
    "case_name = 'Case3'\n",
    "\n",
    "x_1_list = []\n",
    "x_2_list = []\n",
    "x_3_list = []\n",
    "x_4_list = []\n",
    "x_5_list = []\n",
    "obj_list = []\n",
    "iterations=[]\n",
    "\n",
    "runs_number = 3\n",
    "itera = 0\n",
    "from sko.GA import GA\n",
    "from sko.operators import crossover\n",
    "\n",
    "def func(x):\n",
    "    global itera\n",
    "    obj, c1, c2, c3, c4 = Case3(x)\n",
    "    cx1 = -c1-1\n",
    "    cx11=c1-2\n",
    "    cx2=-1-c2\n",
    "    cx22=c2-4\n",
    "    cx3=-2-c3\n",
    "    cx33=c3-3\n",
    "    cx4=-c4\n",
    "    cx44=c4-1.5\n",
    "    itera=itera+1\n",
    "    return obj,cx1,cx2,cx3,cx4,cx11,cx22,cx33,cx44\n",
    "\n",
    "if re_run == True or re_run_save == True:\n",
    "    for num in range(runs_number):\n",
    "        ga = GA(func=lambda x:func(x)[0], n_dim=5, size_pop=100, max_iter=200,lb =[-5, -20,-10,-2,-15] ,ub=[12,22,10,20,18], precision=1e-7,prob_mut = 0.001, constraint_ueq=[lambda x:func(x)[i] for i in range(1,9)])\n",
    "        ga.register(operator_name='crossover', operator=crossover.crossover_2point_prob, crossover_prob=0.5)\n",
    "        best_x, best_y = ga.run()\n",
    "        print( 'obj =', best_y, 'x = ', best_x)\n",
    "\n",
    "        if len(iterations)==0:\n",
    "            iterations.append(itera)\n",
    "        else:\n",
    "            iterations.append(itera-sum(iterations))\n",
    "        \n",
    "        obj_list.append(best_y[0])\n",
    "        x_1_list.append(best_x[0])\n",
    "        x_2_list.append(best_x[1])\n",
    "        x_3_list.append(best_x[2])\n",
    "        x_4_list.append(best_x[3])\n",
    "        x_5_list.append(best_x[4])\n",
    "\n",
    "\n",
    "        df = pd.DataFrame({\n",
    "        'obj': obj_list,\n",
    "        'x_1': x_1_list,\n",
    "        'x_2': x_2_list,\n",
    "        'x_3': x_3_list,\n",
    "        'x_4': x_4_list,\n",
    "        'x_5': x_5_list, \n",
    "        'iterations': iterations\n",
    "        })\n",
    "        if re_run_save == True:\n",
    "            df.to_csv(DIR/f'{sol}_Results_{case_name}_{runs_number}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_count = df['obj'].isna().sum()\n",
    "print(f\"NaN count in column 'obj': {nan_count}\")\n",
    "df.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ASBO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
